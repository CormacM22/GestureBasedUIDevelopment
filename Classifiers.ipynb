{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture UI Project 1 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Option 1: Human Activity Recognition Using Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define File Paths and Load Feature Names\n",
    "\n",
    "In this step, we define the file paths for the dataset, targets, and features files:\n",
    "- `dataset_file`: Path to the dataset file (`dataset.txt`).\n",
    "- `targets_file`: Path to the target labels file (`targets.txt`).\n",
    "- `features_file`: Path to the feature names file (`features.txt`).\n",
    "\n",
    "We then load the feature names from `features.txt`:\n",
    "- The file is read line by line.\n",
    "- Each line is stripped of leading/trailing whitespace and split by spaces.\n",
    "- The second element (index `1`) of each line is extracted as the feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# file paths\n",
    "dataset_file = \"dataset.txt\"\n",
    "targets_file = \"targets.txt\"\n",
    "features_file = \"features.txt\"\n",
    "\n",
    "# Load feature names\n",
    "with open(features_file, \"r\") as f:\n",
    "    feature_names = [line.strip().split(\" \")[1] for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(\"Labels loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "In this step, we load the dataset from the file specified by `dataset_file`. The dataset is read using `pandas.read_csv` with the following parameters:\n",
    "- `delim_whitespace=True`: Indicates that the file is space-separated.\n",
    "- `header=None`: Specifies that the file does not have a header row.\n",
    "- `names=unique_feature_names`: Assigns the unique feature names (previously loaded) as column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_features = pd.read_csv(dataset_file, delim_whitespace=True, header=None, names=unique_feature_names)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df_features.shape}\")  # Verify correct shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure Unique Feature Names\n",
    "\n",
    "In this step, we ensure that all feature names in the dataset are unique. This is important because duplicate feature names can cause issues during data processing and modeling.\n",
    "\n",
    "1. **Count Feature Occurrences**:\n",
    "   - We use `Counter` from the `collections` module to count how many times each feature name appears in the list of feature names.\n",
    "\n",
    "2. **Generate Unique Names**:\n",
    "   - For features that appear more than once, we append a suffix (e.g., `_1`, `_2`, etc.) to make them unique.\n",
    "   - Features that appear only once are kept as is.\n",
    "\n",
    "3. **Assign Unique Names to the Dataset**:\n",
    "   - The updated unique feature names are assigned as the column names of the dataset (`df_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Ensure unique feature names\n",
    "feature_counts = Counter(feature_names)\n",
    "unique_feature_names = []\n",
    "feature_seen = {}\n",
    "\n",
    "for feature in feature_names:\n",
    "    if feature_counts[feature] > 1:\n",
    "        feature_seen[feature] = feature_seen.get(feature, 0) + 1\n",
    "        unique_feature_names.append(f\"{feature}_{feature_seen[feature]}\")\n",
    "    else:\n",
    "        unique_feature_names.append(feature)\n",
    "\n",
    "# Assign new unique names to the dataset\n",
    "df_features.columns = unique_feature_names\n",
    "\n",
    "print(\"Duplicate feature names resolved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Activity Labels and Merge with Features\n",
    "\n",
    "In this step, we load the activity labels from the `targets.txt` file and merge them with the feature dataset (`df_features`) to create a complete dataset.\n",
    "\n",
    "1. **Load Activity Labels**:\n",
    "   - The activity labels are loaded from `targets.txt` using `pandas.read_csv`.\n",
    "   - The file is space-separated (`delim_whitespace=True`), and the column is named `\"Activity\"`.\n",
    "\n",
    "2. **Merge Features and Labels**:\n",
    "   - The feature dataset (`df_features`) and the activity labels (`df_labels`) are merged along the columns (`axis=1`) to create a single DataFrame (`df`).\n",
    "\n",
    "3. **Preview the Dataset**:\n",
    "   - We print the first few rows of the merged dataset to verify that the merge was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load activity labels\n",
    "df_labels = pd.read_csv(targets_file, delim_whitespace=True, header=None, names=[\"Activity\"])\n",
    "\n",
    "# Merge features and labels\n",
    "df = pd.concat([df_features, df_labels], axis=1)\n",
    "\n",
    "print(\"Dataset and labels merged successfully!\")\n",
    "print(df.head())  # Preview dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Features and Target Labels, and Split the Dataset\n",
    "\n",
    "In this step, we prepare the dataset for training and testing by defining the features (`X`) and target labels (`y`), and then splitting the data into training and testing sets.\n",
    "\n",
    "1. **Define Features (`X`) and Target Labels (`y`)**:\n",
    "   - `X` contains all the feature columns, excluding the `\"Activity\"` column.\n",
    "   - `y` contains the target labels, which are the values from the `\"Activity\"` column.\n",
    "\n",
    "2. **Split the Dataset**:\n",
    "   - The dataset is split into training and testing sets using an 80-20 split:\n",
    "     - 80% of the data is used for training (`X_train`, `y_train`).\n",
    "     - 20% of the data is used for testing (`X_test`, `y_test`).\n",
    "   - The `stratify=y` parameter ensures that the class distribution is preserved in both the training and testing sets.\n",
    "   - The `random_state=42` parameter ensures reproducibility of the split.\n",
    "\n",
    "3. **Print Dataset Shapes**:\n",
    "   - The shapes of the training and testing sets are printed to verify the split.\n",
    "\n",
    "This step prepares the data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define features (X) and target labels (y)\n",
    "X = df.drop(columns=[\"Activity\"])  # Feature columns\n",
    "y = df[\"Activity\"]  # Target labels\n",
    "\n",
    "# Split into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training Set: {X_train.shape}, Test Set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Class Distribution in Training and Test Sets\n",
    "\n",
    "In this step, we verify the distribution of classes in both the training and testing sets to ensure that the dataset split has preserved the class proportions.\n",
    "\n",
    "1. **Training Set Distribution**:\n",
    "   - We use `value_counts()` to count the number of instances for each class in the training set (`y_train`).\n",
    "   - The class distribution is printed to confirm that the training set has a balanced representation of all classes.\n",
    "\n",
    "2. **Test Set Distribution**:\n",
    "   - Similarly, we use `value_counts()` to count the number of instances for each class in the test set (`y_test`).\n",
    "\n",
    "This step ensures that the dataset split (using `stratify=y`) has successfully maintained the same class distribution in both the training and testing sets, which is crucial for fair model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check class distribution in training set\n",
    "train_distribution = y_train.value_counts()\n",
    "print(\"Training Set Distribution:\\n\", train_distribution)\n",
    "\n",
    "# Check class distribution in test set\n",
    "test_distribution = y_test.value_counts()\n",
    "print(\"Test Set Distribution:\\n\", test_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features Using StandardScaler\n",
    "\n",
    "In this step, we standardize the feature data to ensure that all features have a mean of 0 and a standard deviation of 1. This is important because many machine learning algorithms perform better when the input features are on the same scale.\n",
    "\n",
    "1. **Initialize the Scaler**:\n",
    "   - We use `StandardScaler` from `sklearn.preprocessing` to standardize the features.\n",
    "\n",
    "2. **Fit and Transform the Training Data**:\n",
    "   - The scaler is fitted to the training data (`X_train`) to compute the mean and standard deviation.\n",
    "   - The training data is then transformed using the computed mean and standard deviation.\n",
    "\n",
    "3. **Transform the Test Data**:\n",
    "   - The test data (`X_test`) is transformed using the same mean and standard deviation computed from the training data. This ensures that the test data is scaled consistently with the training data.\n",
    "\n",
    "\n",
    "Standardizing the features helps improve the performance and convergence of many machine learning models, especially those that are sensitive to the scale of the input data (e.g., SVM, logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data, then transform the test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features standardized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Scaled Data and Labels\n",
    "\n",
    "In this step, we verify the scaled training data and the corresponding labels to ensure that the standardization process has been applied correctly.\n",
    "\n",
    "1. **Check Scaled Training Data**:\n",
    "   - We print the first 5 rows of the scaled training data (`X_train_scaled`) to confirm that the features have been standardized (mean = 0, standard deviation = 1).\n",
    "\n",
    "2. **Check Corresponding Labels**:\n",
    "   - We print the first 5 labels (`y_train`) to verify that they match the scaled training data.\n",
    "\n",
    "This step ensures that the data preprocessing steps (e.g., standardization) have been applied correctly and that the data is ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(X_train_scaled[:5])  # Check first 5 rows of scaled training data\n",
    "print(y_train[:5])  # Check first 5 labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model\n",
    "\n",
    "In this step, we initialize and train a Logistic Regression model using the standardized training data.\n",
    "\n",
    "1. **Initialize the Model**:\n",
    "   - We use `LogisticRegression` from `sklearn.linear_model`.\n",
    "   - The `max_iter=2000` parameter ensures that the model converges by allowing up to 2000 iterations.\n",
    "   - The `random_state=42` parameter ensures reproducibility of the results.\n",
    "\n",
    "2. **Train the Model**:\n",
    "   - The model is trained on the scaled training data (`X_train_scaled`) and the corresponding labels (`y_train`).\n",
    "\n",
    "Logistic Regression is a commonly used classification algorithm that works well for binary and multi-class classification tasks. Standardizing the features ensures that the model performs optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Logistic Regression model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions Using the Logistic Regression Model\n",
    "\n",
    "In this step, we use the trained Logistic Regression model to make predictions on the scaled test data.\n",
    "\n",
    "1. **Generate Predictions**:\n",
    "   - The `predict` method is used to generate predictions (`y_pred_lr`) for the test data (`X_test_scaled`).\n",
    "\n",
    "These predictions will be used to evaluate the performance of the Logistic Regression model by comparing them to the actual labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predictions made successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression Model\n",
    "\n",
    "In this step, we evaluate the performance of the Logistic Regression model using various evaluation metrics.\n",
    "\n",
    "1. **Compute Evaluation Metrics**:\n",
    "   - **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "   - **Precision**: Measures the proportion of true positive predictions among all positive predictions (weighted average is used for multi-class classification).\n",
    "   - **Recall**: Measures the proportion of true positives identified correctly (weighted average is used for multi-class classification).\n",
    "   - **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two (weighted average is used for multi-class classification).\n",
    "\n",
    "2. **Print the Classification Report**:\n",
    "   - The `classification_report` function provides a detailed breakdown of precision, recall, F1-score, and support for each class.\n",
    "\n",
    "3. **Print Evaluation Scores**:\n",
    "   - The accuracy, precision, recall, and F1-score are printed to summarize the model's performance.\n",
    "\n",
    "These metrics help us understand how well the Logistic Regression model is performing on the test data and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average=\"weighted\")\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average=\"weighted\")\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average=\"weighted\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Logistic Regression Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Print evaluation scores\n",
    "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "print(f\"Precision: {precision_lr:.4f}\")\n",
    "print(f\"Recall: {recall_lr:.4f}\")\n",
    "print(f\"F1-Score: {f1_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Model\n",
    "\n",
    "In this step, we initialize and train a Random Forest model using the standardized training data.\n",
    "\n",
    "1. **Initialize the Model**:\n",
    "   - We use `RandomForestClassifier` from `sklearn.ensemble`.\n",
    "   - The `n_estimators=100` parameter specifies the number of trees in the forest.\n",
    "   - The `random_state=42` parameter ensures reproducibility of the results.\n",
    "\n",
    "2. **Train the Model**:\n",
    "   - The model is trained on the scaled training data (`X_train_scaled`) and the corresponding labels (`y_train`).\n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to improve accuracy and reduce overfitting. It is robust and works well for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Random Forest model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions Using the Random Forest Model\n",
    "\n",
    "In this step, we use the trained Random Forest model to make predictions on the scaled test data.\n",
    "\n",
    "1. **Generate Predictions**:\n",
    "   - The `predict` method is used to generate predictions (`y_pred_rf`) for the test data (`X_test_scaled`).\n",
    "\n",
    "These predictions will be used to evaluate the performance of the Random Forest model by comparing them to the actual labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predictions made successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Random Forest Model\n",
    "\n",
    "In this step, we evaluate the performance of the Random Forest model using various evaluation metrics.\n",
    "\n",
    "1. **Compute Evaluation Metrics**:\n",
    "   - **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "   - **Precision**: Measures the proportion of true positive predictions among all positive predictions (weighted average is used for multi-class classification).\n",
    "   - **Recall**: Measures the proportion of true positives identified correctly (weighted average is used for multi-class classification).\n",
    "   - **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two (weighted average is used for multi-class classification).\n",
    "\n",
    "2. **Print the Classification Report**:\n",
    "   - The `classification_report` function provides a detailed breakdown of precision, recall, F1-score, and support for each class.\n",
    "\n",
    "3. **Print Evaluation Scores**:\n",
    "   - The accuracy, precision, recall, and F1-score are printed to summarize the model's performance.\n",
    "\n",
    "These metrics help us understand how well the Random Forest model is performing on the test data and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average=\"weighted\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Random Forest Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Print evaluation scores\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1-Score: {f1_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Support Vector Machine (SVM) Model\n",
    "\n",
    "In this step, we initialize and train a Support Vector Machine (SVM) model using the standardized training data.\n",
    "\n",
    "1. **Initialize the Model**:\n",
    "   - We use `SVC` (Support Vector Classification) from `sklearn.svm`.\n",
    "   - The `kernel=\"linear\"` parameter specifies that a linear kernel should be used, which is suitable for linear classification tasks.\n",
    "   - The `random_state=42` parameter ensures reproducibility of the results.\n",
    "\n",
    "2. **Train the Model**:\n",
    "   - The model is trained on the scaled training data (`X_train_scaled`) and the corresponding labels (`y_train`).\n",
    "\n",
    "SVM is a powerful classification algorithm that works well for both linear and non-linear data (using different kernels). Standardizing the features ensures that the model performs optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42) \n",
    "\n",
    "# Train the model on the training set\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"SVM model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions Using the SVM Model\n",
    "\n",
    "In this step, we use the trained Support Vector Machine (SVM) model to make predictions on the scaled test data.\n",
    "\n",
    "1. **Generate Predictions**:\n",
    "   - The `predict` method is used to generate predictions (`y_pred_svm`) for the test data (`X_test_scaled`).\n",
    "\n",
    "These predictions will be used to evaluate the performance of the SVM model by comparing them to the actual labels (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predictions made successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SVM Model\n",
    "\n",
    "In this step, we evaluate the performance of the Support Vector Machine (SVM) model using various evaluation metrics.\n",
    "\n",
    "1. **Compute Evaluation Metrics**:\n",
    "   - **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "   - **Precision**: Measures the proportion of true positive predictions among all positive predictions (weighted average is used for multi-class classification).\n",
    "   - **Recall**: Measures the proportion of true positives identified correctly (weighted average is used for multi-class classification).\n",
    "   - **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two (weighted average is used for multi-class classification).\n",
    "\n",
    "2. **Print the Classification Report**:\n",
    "   - The `classification_report` function provides a detailed breakdown of precision, recall, F1-score, and support for each class.\n",
    "\n",
    "3. **Print Evaluation Scores**:\n",
    "   - The accuracy, precision, recall, and F1-score are printed to summarize the model's performance.\n",
    "\n",
    "These metrics help us understand how well the SVM model is performing on the test data and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm, average=\"weighted\")\n",
    "recall_svm = recall_score(y_test, y_pred_svm, average=\"weighted\")\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average=\"weighted\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"ðŸ“Š SVM Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Print evaluation scores\n",
    "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
    "print(f\"Precision: {precision_svm:.4f}\")\n",
    "print(f\"Recall: {recall_svm:.4f}\")\n",
    "print(f\"F1-Score: {f1_svm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance\n",
    "\n",
    "In this step, we compare the performance of the three models (Logistic Regression, Random Forest, and SVM) by creating a summary DataFrame of their evaluation metrics.\n",
    "\n",
    "1. **Create a Results DataFrame**:\n",
    "   - A DataFrame is created to store the evaluation metrics (accuracy, precision, recall, and F1-score) for each model.\n",
    "   - The models included are:\n",
    "     - Logistic Regression\n",
    "     - Random Forest\n",
    "     - Support Vector Machine (SVM)\n",
    "\n",
    "2. **Display the Results**:\n",
    "   - The DataFrame is printed to provide a clear comparison of the models' performance.\n",
    "\n",
    "This comparison helps us identify which model performs best on the test data based on the selected evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a DataFrame to compare results\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"SVM\"],\n",
    "    \"Accuracy\": [accuracy_lr, accuracy_rf, accuracy_svm],\n",
    "    \"Precision\": [precision_lr, precision_rf, precision_svm],\n",
    "    \"Recall\": [recall_lr, recall_rf, recall_svm],\n",
    "    \"F1-Score\": [f1_lr, f1_rf, f1_svm]\n",
    "})\n",
    "\n",
    "# Display results\n",
    "print(\"Model Comparison:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Collect results into a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"SVM\"],\n",
    "    \"Accuracy\": [accuracy_lr, accuracy_rf, accuracy_svm],\n",
    "    \"Precision\": [precision_lr, precision_rf, precision_svm],\n",
    "    \"Recall\": [recall_lr, recall_rf, recall_svm],\n",
    "    \"F1-Score\": [f1_lr, f1_rf, f1_svm]\n",
    "})\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "results.set_index(\"Model\").plot(kind=\"bar\", figsize=(10,6), colormap=\"coolwarm\")\n",
    "plt.title(\"Classifier Performance Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Model Performance\n",
    "\n",
    "The table below compares the performance of three models (Logistic Regression, Random Forest, and SVM) based on accuracy, precision, recall, and F1-score:\n",
    "\n",
    "| Model               | Accuracy  | Precision  | Recall     | F1-Score  |\n",
    "|---------------------|-----------|------------|------------|-----------|\n",
    "| Logistic Regression | 0.984466  | 0.984492   | 0.984466   | 0.984468  |\n",
    "| Random Forest       | 0.981553  | 0.981606   | 0.981553   | 0.981548  |\n",
    "| SVM                 | 0.986408  | 0.986424   | 0.986408   | 0.986409  |\n",
    "\n",
    "#### Key Observations:\n",
    "1. **SVM** performs the best across all metrics, achieving the highest accuracy, precision, recall, and F1-score.\n",
    "2. **Logistic Regression** follows closely, with slightly lower scores compared to SVM.\n",
    "3. **Random Forest** has the lowest scores among the three models but still performs very well, with all metrics above 0.98.\n",
    "\n",
    "These results suggest that SVM is the most effective model for this dataset, but all three models demonstrate strong performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
